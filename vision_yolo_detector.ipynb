{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lawesworks/vision-model-workbench/blob/main/vision_yolo_detector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "# CONFIGURATION\n",
        "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "# interesting note..  with TP4 (GPU), training time is about 12min for 30 epochs..  with CPU only, training is nearly 10 hours.\n",
        "\n",
        "# User Settings\n",
        "SAMPLE_IMAGE = \"fire.jpg\"\n",
        "\n",
        "# Roboflow Model Settings\n",
        "Roboflow_Workspace_Name = \"-jwzpw\"\n",
        "Roboflow_Project_Name = \"continuous_fire\"\n",
        "Roboflow_Project_Version = 1\n",
        "\n",
        "# YOLO Model Settings\n",
        "YOLO_Model_Version = \"yolov8\"\n",
        "YOLO_Model_Size = \"n\"\n",
        "\n",
        "# Training Hyper-parameters\n",
        "Config_Epochs = 30\n",
        "Config_Image_Size = 640\n",
        "Config_Batch_Size = 16\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "# Auto-Derived Parameters\n",
        "Roboflow_Project_Folder = Roboflow_Project_Name+\"-\"+str(Roboflow_Project_Version)\n",
        "LATEST_PREDICT_DIR = \"runs/detect/predict\"\n",
        "LATEST_TRAIN_DIR   = \"runs/detect/train\"\n",
        "\n",
        "\n",
        "print(f\"\"\"\n",
        "===== Training Configuration =====\n",
        "\n",
        "Workspace        : {Roboflow_Workspace_Name}\n",
        "Project          : {Roboflow_Project_Name}\n",
        "Project Folder   : {Roboflow_Project_Folder}\n",
        "Dataset Version  : {Roboflow_Project_Version}\n",
        "\n",
        "Model            : {YOLO_Model_Version}\n",
        "Model Size       : {YOLO_Model_Size}\n",
        "Epochs           : {Config_Epochs}\n",
        "Image Size       : {Config_Image_Size}\n",
        "Batch Size       : {Config_Batch_Size}\n",
        "\n",
        "==================================\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "Z-QoqwJCGNKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "# Load Roboflow API Key\n",
        "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "from google.colab import userdata\n",
        "api_key = userdata.get('ROBOFLOW_API_KEY')\n",
        "\n",
        "if api_key is None:\n",
        "    raise ValueError(\"ROBOFLOW_API_KEY not found. Check Colab Secrets.\")\n",
        "\n",
        "print(\"Roboflow API Key Found\")"
      ],
      "metadata": {
        "id": "iNh5qUHtw-0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "# Load Libraries\n",
        "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "from IPython.display import Image, display\n",
        "import glob\n",
        "import os\n",
        "\n",
        "print(f\"\"\"\n",
        "===== Imported Libraries =====\n",
        "\n",
        "Ipython.display  : Image\n",
        "Ipython.display  : display\n",
        "glob\n",
        "os\n",
        "\n",
        "==================================\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "cs7PneItRgBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsEPEvsBf_rm"
      },
      "outputs": [],
      "source": [
        "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "# Install Ultralytics\n",
        "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "print(\"Installing Ultralytics (Please wait)\\n\")\n",
        "\n",
        "!pip install -q roboflow ultralytics\n",
        "\n",
        "print(\"\\nCompleted Ultralytics Install\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "# HELPER FUNCTIONS\n",
        "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from IPython.display import Image, display\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------------------------------\n",
        "# Image Grid (Left ‚Üí Right, Row Wrap) that will create a grid of Images saved to the Predicted Folder\n",
        "# ----------------------------------------------------------------------------------------------------\n",
        "import torch\n",
        "\n",
        "def get_gpu_cv_summary(device_index: int = 0) -> dict:\n",
        "    \"\"\"\n",
        "    Return a concise GPU capability summary for CV / YOLO-style workloads.\n",
        "\n",
        "    Returns a dict with:\n",
        "      - property\n",
        "      - value\n",
        "      - why_it_matters\n",
        "    \"\"\"\n",
        "    if not torch.cuda.is_available():\n",
        "        raise RuntimeError(\"CUDA is not available. Enable a GPU runtime or run on a CUDA-capable machine.\")\n",
        "\n",
        "    props = torch.cuda.get_device_properties(device_index)\n",
        "\n",
        "    # Some attributes may not exist on all builds/versions; use getattr with fallback.\n",
        "    summary = [\n",
        "        {\n",
        "            \"property\": \"name\",\n",
        "            \"value\": props.name,\n",
        "            \"why_it_matters\": \"GPU generation & capabilities\",\n",
        "        },\n",
        "        {\n",
        "            \"property\": \"total_memory\",\n",
        "            \"value\": f\"{props.total_memory / (1024**3):.2f} GB\",\n",
        "            \"why_it_matters\": \"Max batch size & image resolution\",\n",
        "        },\n",
        "        {\n",
        "            \"property\": \"multi_processor_count\",\n",
        "            \"value\": props.multi_processor_count,\n",
        "            \"why_it_matters\": \"Parallel throughput\",\n",
        "        },\n",
        "        {\n",
        "            \"property\": \"clock_rate\",\n",
        "            \"value\": f\"{getattr(props, 'clock_rate', None) / 1000:.0f} MHz\"\n",
        "                     if getattr(props, \"clock_rate\", None) is not None else \"N/A\",\n",
        "            \"why_it_matters\": \"Kernel execution speed\",\n",
        "        },\n",
        "        {\n",
        "            \"property\": \"memory_bus_width\",\n",
        "            \"value\": getattr(props, \"memory_bus_width\", \"N/A\"),\n",
        "            \"why_it_matters\": \"Data movement speed\",\n",
        "        },\n",
        "        {\n",
        "            \"property\": \"warp_size\",\n",
        "            \"value\": props.warp_size,\n",
        "            \"why_it_matters\": \"Kernel efficiency\",\n",
        "        },\n",
        "        {\n",
        "            \"property\": \"(major.minor)\",\n",
        "            \"value\": f\"{props.major}.{props.minor}\",\n",
        "            \"why_it_matters\": \"CUDA feature support / compute_capability \",\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    return {\"device_index\": device_index, \"gpu_summary\": summary}\n",
        "\n",
        "\n",
        "def print_gpu_cv_summary(device_index: int = 0) -> None:\n",
        "    \"\"\"\n",
        "    Pretty-print the GPU summary in a readable table-like format.\n",
        "    \"\"\"\n",
        "    result = get_gpu_cv_summary(device_index)\n",
        "    rows = result[\"gpu_summary\"]\n",
        "\n",
        "    print(f\"\\nGPU CV/YOLO Capability Summary (device {result['device_index']})\")\n",
        "    print(\"-\" * 78)\n",
        "    print(f\"{'Property':<28} {'Value':<20} {'Why it matters'}\")\n",
        "    print(\"-\" * 78)\n",
        "    for r in rows:\n",
        "        print(f\"{r['property']:<28} {str(r['value']):<20} {r['why_it_matters']}\")\n",
        "    print(\"-\" * 78)\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------------------------------\n",
        "# Image Grid (Left ‚Üí Right, Row Wrap) that will create a grid of Images saved to the Predicted Folder\n",
        "# ----------------------------------------------------------------------------------------------------\n",
        "def show_image_grid_paged(image_paths, cols=5, per_page=20, page=1, figsize_per_cell=3):\n",
        "    \"\"\"\n",
        "    Display images in a true grid, paged.\n",
        "    - cols: images per row\n",
        "    - per_page: total images per page\n",
        "    - page: 1-based page index\n",
        "    - figsize_per_cell: size multiplier per grid cell\n",
        "    \"\"\"\n",
        "    if not image_paths:\n",
        "        print(\"No images to display.\")\n",
        "        return\n",
        "\n",
        "    start = (page - 1) * per_page\n",
        "    end = min(start + per_page, len(image_paths))\n",
        "    page_paths = image_paths[start:end]\n",
        "\n",
        "    rows = math.ceil(len(page_paths) / cols)\n",
        "    fig_w = cols * figsize_per_cell\n",
        "    fig_h = rows * figsize_per_cell\n",
        "\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(fig_w, fig_h))\n",
        "    axes = axes.flatten() if isinstance(axes, (list, tuple)) is False else axes\n",
        "\n",
        "    # If only one subplot, axes may not be iterable the same way\n",
        "    try:\n",
        "        axes = axes.flatten()\n",
        "    except Exception:\n",
        "        axes = [axes]\n",
        "\n",
        "    for ax in axes:\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "    for ax, img_path in zip(axes, page_paths):\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        ax.imshow(img)\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Showing images {start+1}‚Äì{end} of {len(image_paths)} (page {page})\")\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------------------------------\n",
        "# Filenames are fine, but only if you keep them small and short - this version truncates titles\n",
        "# ----------------------------------------------------------------------------------------------------\n",
        "def show_image_grid_with_short_titles(image_paths, cols=5, per_page=20, page=1, figsize_per_cell=3):\n",
        "    #import math\n",
        "    #import matplotlib.pyplot as plt\n",
        "    #from PIL import Image\n",
        "\n",
        "    start = (page - 1) * per_page\n",
        "    end = min(start + per_page, len(image_paths))\n",
        "    page_paths = image_paths[start:end]\n",
        "\n",
        "    rows = math.ceil(len(page_paths) / cols)\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(cols*figsize_per_cell, rows*figsize_per_cell))\n",
        "    try:\n",
        "        axes = axes.flatten()\n",
        "    except Exception:\n",
        "        axes = [axes]\n",
        "\n",
        "    for ax in axes:\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "    for ax, img_path in zip(axes, page_paths):\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        ax.imshow(img)\n",
        "        ax.axis(\"off\")\n",
        "        ax.set_title(os.path.basename(img_path)[:18], fontsize=7)  # truncate title\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print(f\"Showing images {start+1}‚Äì{end} of {len(image_paths)} (page {page})\")\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------------------------------\n",
        "# function to determine estimated time of completion\n",
        "# ----------------------------------------------------------------------------------------------------\n",
        "from datetime import datetime, timedelta\n",
        "from zoneinfo import ZoneInfo\n",
        "\n",
        "def estimate_completion_time(minutes_from_now):\n",
        "    now = datetime.now(ZoneInfo(\"America/New_York\"))\n",
        "    eta = now + timedelta(minutes=minutes_from_now)\n",
        "\n",
        "    return {\n",
        "        \"start_time\": now.strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        \"estimated_completion\": eta.strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        \"minutes_from_now\": minutes_from_now\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------------------------------\n",
        "# function to estimate training time required for the model\n",
        "# ----------------------------------------------------------------------------------------------------\n",
        "def estimate_training_time(\n",
        "    num_epochs,\n",
        "    batch_size,\n",
        "    train_image_count,\n",
        "    val_image_count=0,\n",
        "    seconds_per_batch=None,\n",
        "    seconds_per_epoch=None,\n",
        "    val_fraction_of_train_time=0.3\n",
        "):\n",
        "    \"\"\"\n",
        "    Estimate total training time.\n",
        "\n",
        "    Provide either seconds_per_batch OR seconds_per_epoch (preferrably time for a full epoch).\n",
        "    If both provided, seconds_per_epoch takes precedence.\n",
        "\n",
        "    Returns dict with per-epoch and total estimates.\n",
        "    \"\"\"\n",
        "    if seconds_per_epoch is None and seconds_per_batch is None:\n",
        "        raise ValueError(\"Provide either seconds_per_batch or seconds_per_epoch.\")\n",
        "\n",
        "    train_batches_per_epoch = math.ceil(train_image_count / batch_size)\n",
        "\n",
        "    # If user supplied epoch time, convert to per-batch\n",
        "    if seconds_per_epoch is not None:\n",
        "        seconds_per_batch = seconds_per_epoch / train_batches_per_epoch\n",
        "\n",
        "    train_time_per_epoch = train_batches_per_epoch * seconds_per_batch\n",
        "    val_time_per_epoch = 0\n",
        "    if val_image_count > 0:\n",
        "        # Option: scale validation time by image counts, or use fraction\n",
        "        # Here we estimate validation time proportionally to image counts:\n",
        "        val_batches_per_epoch = math.ceil(val_image_count / batch_size)\n",
        "        val_time_per_epoch = val_batches_per_epoch * seconds_per_batch * val_fraction_of_train_time\n",
        "\n",
        "    total_time_seconds = num_epochs * (train_time_per_epoch + val_time_per_epoch)\n",
        "\n",
        "    return {\n",
        "        \"train_batches_per_epoch\": train_batches_per_epoch,\n",
        "        \"train_time_per_epoch_sec\": round(train_time_per_epoch, 2),\n",
        "        \"val_time_per_epoch_sec\": round(val_time_per_epoch, 2),\n",
        "        \"total_time_sec\": round(total_time_seconds, 2),\n",
        "        \"total_time_min\": round(total_time_seconds / 60, 2),\n",
        "        \"total_time_hr\": round(total_time_seconds / 3600, 2),\n",
        "        \"seconds_per_batch\": round(seconds_per_batch, 4),\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------------------------------\n",
        "# function to count images in given folder\n",
        "# ----------------------------------------------------------------------------------------------------\n",
        "def count_images(folder):\n",
        "    extensions = (\"*.jpg\", \"*.jpeg\", \"*.png\")\n",
        "    count = 0\n",
        "    for ext in extensions:\n",
        "        count += len(glob.glob(os.path.join(folder, ext)))\n",
        "    return count\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------------------------------\n",
        "# get latest training path DIR\n",
        "# ----------------------------------------------------------------------------------------------------\n",
        "def get_latest_training_path():\n",
        "    training_dirs = sorted(\n",
        "        glob.glob(\"/content/runs/detect/train*\"),\n",
        "        key=os.path.getmtime\n",
        "    )\n",
        "\n",
        "    if not training_dirs:\n",
        "        raise FileNotFoundError(\"No YOLO training runs found in runs/detect/\")\n",
        "\n",
        "    train_dir = training_dirs[-1]\n",
        "    print(f\"\\nUsing training run from: {train_dir}\")\n",
        "    return train_dir\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------------------------------\n",
        "# get latest prediction path DIR\n",
        "# ----------------------------------------------------------------------------------------------------\n",
        "def get_latest_prediction_path():\n",
        "  predict_dirs = sorted(\n",
        "    glob.glob(\"runs/detect/predict*\"),\n",
        "    key=os.path.getmtime\n",
        "  )\n",
        "\n",
        "  PREDICT_DIR = predict_dirs[-1]\n",
        "  print(f\"\\n\\nUsing predictions from: {PREDICT_DIR}\")\n",
        "  return PREDICT_DIR\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------------------------------\n",
        "# get latest model best.pt path\n",
        "# ----------------------------------------------------------------------------------------------------\n",
        "def get_latest_pt_path(latest_train_path):\n",
        "  pt_files = glob.glob(f\"{latest_train_path}/weights/best.pt\", recursive=True)\n",
        "\n",
        "  if not pt_files:\n",
        "      raise FileNotFoundError(\"No best.pt file found\")\n",
        "\n",
        "  return pt_files[0]\n",
        "\n",
        "# ----------------------------------------------------------------------------------------------------\n",
        "# get list of predicted / inferenced images\n",
        "# ----------------------------------------------------------------------------------------------------\n",
        "def get_inferenced_images(predict_dir):\n",
        "  image_paths = glob.glob(os.path.join(predict_dir, \"*.jpg\")) + \\\n",
        "              glob.glob(os.path.join(predict_dir, \"*.png\"))\n",
        "\n",
        "  return sorted(image_paths)\n",
        "\n",
        "\n",
        "print(f\"\"\"\n",
        "===== Loaded Helper Functions =====\n",
        "\n",
        "show_image_grid_paged              :\n",
        "show_image_grid_with_short_titles  :\n",
        "estimate_completion_time           :\n",
        "estimate_training_time             :\n",
        "count_images                       :\n",
        "get_latest_training_path           :\n",
        "get_latest_prediction_path         :\n",
        "get_latest_pt_path                 :\n",
        "get_inferenced_images              :\n",
        "\n",
        "==================================\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "zkB7EtWuWLpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "# Load Libraries\n",
        "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "from IPython.display import Image, display\n",
        "import glob\n",
        "import os\n",
        "\n",
        "print(f\"\"\"\n",
        "===== Imported Libraries =====\n",
        "\n",
        "Ipython.display  : Image\n",
        "Ipython.display  : display\n",
        "glob\n",
        "os\n",
        "\n",
        "==================================\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "CU_DAC90WlKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "# Import the Dataset that will be used to train your model\n",
        "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "print(f\"Importing Dataset: [{Roboflow_Project_Name}, Version: [{Roboflow_Project_Version}] from: [{Roboflow_Workspace_Name}] (Please wait)\\n\")\n",
        "\n",
        "from roboflow import Roboflow\n",
        "\n",
        "\n",
        "rf = Roboflow(api_key=api_key )\n",
        "project = rf.workspace(Roboflow_Workspace_Name).project(Roboflow_Project_Name)\n",
        "dataset = project.version(Roboflow_Project_Version).download(YOLO_Model_Version)  # adjust version if needed\n",
        "\n",
        "print(\"\\nCompleted Dataset Import\")\n",
        "\n",
        "Data_Train_Count = count_images(os.path.join(Roboflow_Project_Folder, \"train\", \"images\"))\n",
        "Data_Valid_Count = count_images(os.path.join(Roboflow_Project_Folder, \"valid\", \"images\"))\n",
        "\n",
        "print(f\"\"\"\n",
        "===== Training Data Statistics =====\n",
        "\n",
        "Training Image Count               : {Data_Train_Count}\n",
        "Validation Image Count             : {Data_Valid_Count}\n",
        "\n",
        "====================================\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "rDXMDK-LxKOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "# get location of the yaml file for this project\n",
        "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "yaml_files = glob.glob(f\"/content/{Roboflow_Project_Folder}/data.yaml\", recursive=True)\n",
        "\n",
        "if not yaml_files:\n",
        "    raise FileNotFoundError(\"No data.yaml file found\")\n",
        "\n",
        "DATA_YAML_PATH = yaml_files[0]\n",
        "\n",
        "print(f\"Using dataset config: {DATA_YAML_PATH}\")"
      ],
      "metadata": {
        "id": "DBZ34DOtx5h-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "# Estimate Time Required to Complete Training\n",
        "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "estimate = estimate_training_time(\n",
        "    num_epochs=Config_Epochs,\n",
        "    batch_size=Config_Batch_Size,\n",
        "    train_image_count = Data_Train_Count,\n",
        "    val_image_count = Data_Valid_Count,\n",
        "    seconds_per_epoch=25,   # measured full epoch time (NOT per batch)\n",
        "    val_fraction_of_train_time=0.3\n",
        ")\n",
        "\n",
        "\n",
        "# print time estimates\n",
        "for k, v in estimate.items():\n",
        "    print(f\"{k:<30}: {v}\")\n",
        "\n",
        "# print estimate time of completion\n",
        "eta_info = estimate_completion_time(estimate[\"total_time_min\"])\n",
        "for k, v in eta_info.items():\n",
        "  print(f\"{k:<30}: {v}\")\n",
        "\n",
        "print(\"\\n\")\n",
        "print_gpu_cv_summary(0)\n"
      ],
      "metadata": {
        "id": "L6N1UYiWsfiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!nvidia-smi -q\n",
        "#!nvidia-smi"
      ],
      "metadata": {
        "id": "pz5prOlXNvmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "# train  model on the data\n",
        "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "\n",
        "print(\"Training The Model (Please wait)\\n\")\n",
        "print(f\"\"\"\n",
        "===== Training Hyper-parameters =====\n",
        "\n",
        "Model            : {YOLO_Model_Version}\n",
        "Model Size       : {YOLO_Model_Size}\n",
        "Epochs           : {Config_Epochs}\n",
        "Image Size       : {Config_Image_Size}\n",
        "Batch Size       : {Config_Batch_Size}\n",
        "Train Count      : {Data_Train_Count}\n",
        "Valid Count      : {Data_Valid_Count}\n",
        "\n",
        "=====================================\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "from ultralytics import YOLO\n",
        "\n",
        "DATA_YAML_PATH = DATA_YAML_PATH\n",
        "\n",
        "model = YOLO(\"yolov8n.pt\")  # small + fast starter model\n",
        "results = model.train(\n",
        "    data=DATA_YAML_PATH,\n",
        "    epochs=Config_Epochs,\n",
        "    imgsz=Config_Image_Size,\n",
        "    batch=Config_Batch_Size\n",
        ")\n",
        "\n",
        "# get latest training folder - save it to variabble\n",
        "LATEST_TRAIN_DIR = get_latest_training_path()\n"
      ],
      "metadata": {
        "id": "wKpcyO_Gx_8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "# get location of the weights for the latest training\n",
        "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "\n",
        "MODEL_PT_PATH = get_latest_pt_path(LATEST_TRAIN_DIR)\n",
        "\n",
        "print(f\"Using PT File: {MODEL_PT_PATH}\")"
      ],
      "metadata": {
        "id": "Gpe9lasUznLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "# run the model against the test data in your project\n",
        "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "best_model_path = glob.glob(MODEL_PT_PATH)[-1]\n",
        "model = YOLO(best_model_path)\n",
        "\n",
        "model.predict(source = Roboflow_Project_Folder + \"/test/images\", save=True, conf=0.25)\n",
        "\n",
        "LATEST_PREDICT_DIR = get_latest_prediction_path()"
      ],
      "metadata": {
        "id": "IvhHuMpjzrkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "# Get list of latest predicted / inferenced images\n",
        "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "\n",
        "image_paths = get_inferenced_images(LATEST_PREDICT_DIR)\n",
        "\n",
        "print(f\"Found {len(image_paths)} predicted images\")"
      ],
      "metadata": {
        "id": "nPqkd6LGViPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "# Show images in the Recent Predictions Folder\n",
        "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "# show first 20 images, 5 per row\n",
        "show_image_grid_paged(image_paths, cols=8, per_page=24, page=1)\n"
      ],
      "metadata": {
        "id": "oRYBgbhLXzk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "# Show first 20 images with titles, 5 per row\n",
        "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "show_image_grid_with_short_titles(image_paths, cols=8, per_page=24, page=1)"
      ],
      "metadata": {
        "id": "ZZ5A0bgOZnmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "# View a screenshot of the training plots (Loss Function plots / Accuracy)\n",
        "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "Image(filename=f'/{LATEST_TRAIN_DIR}/results.png', width=600)"
      ],
      "metadata": {
        "id": "fe40sEEG1l0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "# View a screenshot of the Confusio Matrix\n",
        "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "Image(filename=f'/{LATEST_TRAIN_DIR}/confusion_matrix.png', width=600)"
      ],
      "metadata": {
        "id": "-9DKWU2_2NVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "# View training metrics\n",
        "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "# model.val(data=DATA_YAML_PATH) # dumps everything (verbose)\n",
        "\n",
        "metrics = model.val(data=DATA_YAML_PATH)\n",
        "\n",
        "print(\"mAP@0.5      :\", metrics.box.map50)\n",
        "print(\"mAP@0.5:0.95 :\", metrics.box.map)\n",
        "print(\"Precision    :\", metrics.box.mp)\n",
        "print(\"Recall       :\", metrics.box.mr)\n"
      ],
      "metadata": {
        "id": "AJBMfw3H1CyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä Model Evaluation Summary\n",
        "\n",
        "### Overall Assessment\n",
        "The model performed **exceptionally well** on the evaluation dataset. All key detection metrics are high, indicating strong object recognition, reliable predictions, and good localization accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "### Key Metrics\n",
        "\n",
        "- **mAP@0.5:** `0.992`\n",
        "- **mAP@0.5:0.95:** `0.821`\n",
        "- **Precision:** `0.986`\n",
        "- **Recall:** `0.964`\n",
        "\n",
        "---\n",
        "\n",
        "### Metric Interpretation\n",
        "\n",
        "#### **mAP@0.5 ‚Äî Excellent Detection Performance**\n",
        "- Nearly perfect performance at the 0.5 IoU threshold.\n",
        "- The model consistently finds objects and places bounding boxes with sufficient overlap.\n",
        "- Indicates the model has learned the core visual patterns in the dataset extremely well.\n",
        "\n",
        "#### **mAP@0.5:0.95 ‚Äî Strong Localization Accuracy**\n",
        "- Evaluates performance across stricter IoU thresholds.\n",
        "- The expected drop from mAP@0.5 reflects normal tightening of box placement requirements.\n",
        "- Suggests good‚Äîbut not pixel-perfect‚Äîbounding box precision, which is typical and acceptable.\n",
        "\n",
        "#### **Precision ‚Äî Very High Reliability**\n",
        "- When the model predicts an object, it is almost always correct.\n",
        "- Very few false positives.\n",
        "- Indicates conservative and trustworthy predictions.\n",
        "\n",
        "#### **Recall ‚Äî Strong Object Coverage**\n",
        "- The model detects the vast majority of ground-truth objects.\n",
        "- Slightly lower than precision, meaning some difficult cases (small, occluded, low-contrast) may be missed.\n",
        "- Reflects a bias toward avoiding false positives over finding every object.\n",
        "\n",
        "---\n",
        "\n",
        "### Precision vs Recall Balance\n",
        "- **Precision > Recall**  \n",
        "  The model prioritizes correctness over completeness.\n",
        "- This is often desirable in scenarios where false positives are more costly than missed detections.\n",
        "\n",
        "---\n",
        "\n",
        "### What These Results Suggest\n",
        "\n",
        "- The dataset is likely **clean and consistently labeled**.\n",
        "- Transfer learning was effective.\n",
        "- No obvious signs of underfitting.\n",
        "- The model is well-tuned for the evaluated data distribution.\n",
        "\n",
        "---\n",
        "\n",
        "### Important Caveats\n",
        "\n",
        "High metrics alone do not guarantee real-world performance. Consider:\n",
        "- Dataset size and diversity (lighting, angles, environments)\n",
        "- Potential validation data leakage\n",
        "- Performance on truly unseen data\n",
        "\n",
        "---\n",
        "\n",
        "### Final Takeaway\n",
        "> **The model demonstrates outstanding performance on the evaluation dataset, with high confidence, strong detection capability, and reliable localization.  \n",
        "> The next step is to validate robustness using new, unseen data to confirm generalization.\n",
        "> **The model is biased toward ‚Äúdon‚Äôt be wrong,‚Äù rather than ‚Äúfind everything.‚Äù\n",
        "\n",
        "---\n",
        "### Metric Summary\n",
        "\n",
        "| Metric        | Value | Assessment                     |\n",
        "|---------------|-------|--------------------------------|\n",
        "| mAP@0.5       | 0.992 | Outstanding                    |\n",
        "| mAP@0.5:0.95  | 0.821 | Very strong                    |\n",
        "| Precision     | 0.986 | Extremely reliable             |\n",
        "| Recall        | 0.964 | Strong, slightly conservative  |\n"
      ],
      "metadata": {
        "id": "Lh0fNdg4g7de"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "# Run inference on an arbitrary image and save it to the sandbox folder\n",
        "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "# I've provided a URL to an image, but you can just as easily upload a file into Colab contents folder\n",
        "# and use that as your source\n",
        "\n",
        "#model.predict(source=f\"/content/{SAMPLE_IMAGE}\", save=True, conf=0.25)\n",
        "\n",
        "model.predict(\n",
        "    source=\"https://github.com/lawesworks/vision-model-workbench/blob/main/images/home%20fire%20hero.jpg?raw=true\",\n",
        "    save=True,\n",
        "    conf=0.25,\n",
        "    name=f\"sandbox\",\n",
        "    exist_ok=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "4Hcax3A20d_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "# View a screenshot of the predicted annotated result\n",
        "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "# find the saved image (YOLO keeps original name or auto-generated one)\n",
        "predicted_images = glob.glob(os.path.join(\"/content/runs/detect/sandbox\", \"*.jpg\"))\n",
        "\n",
        "# Sort by last modified time (oldest ‚Üí newest)\n",
        "predicted_images.sort(key=os.path.getmtime)\n",
        "latest_image = predicted_images[-1]\n",
        "\n",
        "print(\"Opening:\", latest_image,\"\\n\")\n",
        "\n",
        "Image(\n",
        "    filename=latest_image,\n",
        "    width=600\n",
        ")"
      ],
      "metadata": {
        "id": "Puj_QVjxUYLN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}